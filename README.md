# BEES â€“ Data Engineering Case: Breweries Pipeline

## ğŸ“Œ DescriÃ§Ã£o

Este projeto implementa um pipeline de dados baseado em arquitetura Medallion (bronze, silver, gold), com orquestraÃ§Ã£o via Apache Airflow e execuÃ§Ã£o em ambiente Docker. A fonte de dados Ã© a Open Brewery DB API, contendo informaÃ§Ãµes pÃºblicas sobre cervejarias nos EUA.

## ğŸš€ ExecuÃ§Ã£o

Para rodar o projeto localmente:

```bash
docker-compose up --build
```

Acesse a interface do Airflow em: http://localhost:8080  
UsuÃ¡rio padrÃ£o: `airflow`  
Senha padrÃ£o: `airflow`

## ğŸ—ï¸ Estrutura do Pipeline

1. **Bronze Layer**: Coleta os dados da API e salva em formato JSON, sem transformaÃ§Ãµes.
2. **Silver Layer**: Converte os dados para formato Parquet, particionado por estado da cervejaria.
3. **Gold Layer**: Gera uma visÃ£o analÃ­tica com a contagem de cervejarias por tipo e estado.

## ğŸ“ˆ Monitoramento

O pipeline inclui notificaÃ§Ã£o por e-mail via `EmailOperator` em caso de falha, possibilitando alertas rÃ¡pidos. Para habilitar o envio, configure as credenciais SMTP no backend do Airflow.

## ğŸ§ª Testes

A aplicaÃ§Ã£o conta com testes unitÃ¡rios para as etapas de extraÃ§Ã£o e transformaÃ§Ã£o:

```bash
cd dags/tests
python -m unittest test_fetch.py
python -m unittest test_transform.py
```

## ğŸ“‚ Estrutura do Data Lake

```
data_lake/
â”œâ”€â”€ bronze/        -> Dados crus (JSON)
â”œâ”€â”€ silver/        -> Dados em Parquet particionados por estado
â””â”€â”€ gold/          -> AgregaÃ§Ã£o de cervejarias por tipo e estado
```

## ğŸ“Š Resultado esperado

ApÃ³s a execuÃ§Ã£o do pipeline no Airflow, os arquivos transformados estarÃ£o salvos localmente na pasta `data_lake`, organizados por camada.

